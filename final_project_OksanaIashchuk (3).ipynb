{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "078811b1",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Deep Learning for Computer Vision and NLP 2025\n",
    "</h1>\n",
    "<h2 style=\"text-align: center;\">Final Project</h2>\n",
    "<h2 style=\"text-align: center;\">–ì–æ–ª–æ–≤–Ω–∞ –º–µ—Ç–∞ —Ñ—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ—î–∫—Ç—É –∫—É—Ä—Å—É - –ø–µ—Ä–µ–¥–±–∞—á–∏—Ç–∏ —à–≤–∏–¥–∫—ñ—Å—Ç—å –∞–¥–æ–ø—Ü—ñ—ó –¥–æ–º–∞—à–Ω—ñ—Ö —Ç–≤–∞—Ä–∏–Ω –Ω–∞ –æ—Å–Ω–æ–≤—ñ —ó—Ö–Ω—ñ—Ö –ø—Ä–æ—Ñ—ñ–ª—ñ–≤ –Ω–∞ PetFinder.my.</h2>\n",
    "<h2 style=\"text-align: center;\">—Ä–æ–±–æ—Ç–∞ –∑ –±–∞–∑–æ—é –¥–∞–Ω–∏—Ö PetFinder.my, –Ω–∞–π–±—ñ–ª—å—à–æ–≥–æ –æ–Ω–ª–∞–π–Ω-—Å–µ—Ä–≤—ñ—Å—É –∑ —É—Å–∏–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–≤–∞—Ä–∏–Ω —É –ú–∞–ª–∞–π–∑—ñ—ó</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e23708",
   "metadata": {},
   "source": [
    "üé¢ –í–∞—à–∞ –∑–∞–¥–∞—á–∞ –ø–æ–ª—è–≥–∞—î –≤ —Ä–æ–∑—Ä–æ–±—Ü—ñ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—ó –ø—Ä–æ–≥–Ω–æ–∑–Ω–æ—ó –º–æ–¥–µ–ª—ñ, —è–∫–∞ –∑–º–æ–∂–µ –æ–±—Ä–æ–±–ª—è—Ç–∏ –≤–µ–ª–∏–∫—É –∫—ñ–ª—å–∫—ñ—Å—Ç—å –≤—Ö—ñ–¥–Ω–∏—Ö –æ–∑–Ω–∞–∫. –í–∏ –º–∞—î—Ç–µ –ø–æ–∫–∞–∑–∞—Ç–∏ —Å–≤–æ—é –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –∑ —Ç–µ–∫—Å—Ç–æ–≤–∏–º–∏ –¥–∞–Ω–∏–º–∏ —ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è–º–∏.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9dc5a",
   "metadata": {},
   "source": [
    "Description\n",
    "link\n",
    "keyboard_arrow_up\n",
    "–ü—Ä–æ–≥–Ω–æ–∑–∏ –≤–∞—à–æ—ó –º–æ–¥–µ–ª—ñ –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ–≥–æ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö –æ—Ü—ñ–Ω—é–≤–∞—Ç–∏–º—É—Ç—å—Å—è –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –º–µ—Ç—Ä–∏–∫–∏ ‚Äúquadratic weighted kappa‚Äù.\n",
    "\n",
    "–ü—Ä–æ—Ü–µ—Å –≤–∏—Å—Ç–∞–≤–ª–µ–Ω–Ω—è –±–∞–ª—ñ–≤:\n",
    "\n",
    "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π –±–∞–ª (100) –ø—Ä–∏—Å–≤–æ—é—î—Ç—å—Å—è, —è–∫—â–æ —Å—Ç—É–¥–µ–Ω—Ç –Ω–∞–±–∏—Ä–∞—î 0.5 –∞–±–æ –±—ñ–ª—å—à–µ –Ω–∞ –º–µ—Ç—Ä–∏—Ü—ñ –∑–º–∞–≥–∞–Ω–Ω—è.\n",
    "–Ø–∫—â–æ —Å—Ç—É–¥–µ–Ω—Ç –Ω–∞–±–∏—Ä–∞—î –º–µ–Ω—à–µ 0.5, –±–∞–ª–∏ —Ä–æ–∑—Ä–∞—Ö–æ–≤—É—é—Ç—å—Å—è –ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–æ.\n",
    "–§–æ—Ä–º—É–ª–∞ –¥–ª—è —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É –±–∞–ª—ñ–≤, —è–∫—â–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∏–∂—á–µ 0.5:\n",
    "*–ë–∞–ª = (–†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ç—É–¥–µ–Ω—Ç–∞ √ó 100 / 50) *\n",
    "\n",
    "–¢–∞–∫–æ–∂, –∫–æ–∂–µ–Ω —Å—Ç—É–¥–µ–Ω—Ç, —è–∫–∏–π –æ–ø—É–±–ª—ñ–∫—É—î –∫–æ–¥ —Å–≤–æ–≥–æ —Ä–æ–∑–≤‚Äô—è–∑–∞–Ω–Ω—è –∑–∞–≤–¥–∞–Ω–Ω—è –∑ –¥–µ—Ç–∞–ª—å–Ω–∏–º –æ–ø–∏—Å–æ–º –∫—Ä–æ–∫—ñ–≤ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è –Ω–∞ —Ñ–æ—Ä—É–º—ñ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ –∑–º–∞–≥–∞–Ω—å –ø—Ä–æ—Ç—è–≥–æ–º –¥–≤–æ—Ö –¥–Ω—ñ–≤ —ñ–∑ –¥–∞—Ç–∏ –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è –∑–º–∞–≥–∞–Ω—å, –æ—Ç—Ä–∏–º–∞—î –¥–æ–¥–∞—Ç–∫–æ–≤—ñ 10 –±–∞–ª—ñ–≤, –∞–ª–µ –Ω–µ –ø–æ–Ω–∞–¥ –∑–∞–∫–ª–∞–¥–µ–Ω–∏—Ö –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏—Ö 100 –±–∞–ª—ñ–≤ (—Ç–æ–±—Ç–æ —è–∫—â–æ –∑–∞ –≤–∏–∫–æ–Ω–∞–Ω–Ω—è —Ü—ñ—î—ó —É–º–æ–≤–∏ —Å—Ç—É–¥–µ–Ω—Ç –æ—Ç—Ä–∏–º—É—î –ø–æ–Ω–∞–¥ 100 –±., –æ—Ü—ñ–Ω–∫–∞ —Ä–æ–±–æ—Ç–∏ —Å–∫–ª–∞–¥–∞—Ç–∏–º–µ 100 –±.).\n",
    "\n",
    "–©–æ–± —Ä–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–µ—Ç—Ä–∏–∫–∏ –±—É–≤ –∫–æ—Ä–µ–∫—Ç–Ω–∏–º, –≤–∞–º –ø–æ—Ç—Ä—ñ–±–Ω–æ —Å—Ñ–æ—Ä–º—É–≤–∞—Ç–∏ —Ñ–∞–π–ª .csv –∑ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–æ–º —Ç–≤–∞—Ä–∏–Ω–∏ —Ç–∞ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–æ—é —à–≤–∏–¥–∫—ñ—Å—Ç—é –∞–¥–æ–ø—Ü—ñ—ó. –ü–æ—Ä—è–¥–æ–∫ —Ä—è–¥–∫—ñ–≤ –Ω–µ –º–∞—î –∑–Ω–∞—á–µ–Ω–Ω—è. –§–∞–π–ª –ø–æ–≤–∏–Ω–µ–Ω –º–∞—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —ñ –≤–∏–≥–ª—è–¥–∞—Ç–∏ —Ç–∞–∫:\n",
    "\n",
    "PetID,AdoptionSpeed\n",
    "378fcc4fc,3\n",
    "73c10e136,2\n",
    "72000c4c5,1\n",
    "e147a4b9f,4\n",
    "etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b16c1",
   "metadata": {},
   "source": [
    "–†–µ—Ç–µ–ª—å–Ω–æ –≤–∏–≤—á—ñ—Ç—å –¥–∞–Ω—ñ: –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞ –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ –Ω–∞–¥–∞–Ω–∏—Ö –∑–æ–±—Ä–∞–∂–µ–Ω—å —ñ —Ç–µ–∫—Å—Ç—ñ–≤.\n",
    "\n",
    "–ü–æ—á–Ω—ñ—Ç—å –∑ –ø—Ä–æ—Å—Ç–æ–≥–æ: —Å—Ç–≤–æ—Ä—ñ—Ç—å –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å —ñ –ø–æ—Å—Ç—É–ø–æ–≤–æ —ó—ó –≤–¥–æ—Å–∫–æ–Ω–∞–ª—é–π—Ç–µ.\n",
    "\n",
    "–ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–π—Ç–µ –∑ —Ä—ñ–∑–Ω–∏–º–∏ –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞–º–∏: —Å–ø—Ä–æ–±—É–π—Ç–µ —Ä—ñ–∑–Ω—ñ –º–æ–¥–µ–ª—ñ –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –∑–æ–±—Ä–∞–∂–µ–Ω—å (CNN, ResNet) —Ç–∞ —Ç–µ–∫—Å—Ç—É (LSTM, Transformers).\n",
    "\n",
    "–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞–≤—á–µ–Ω—ñ –º–æ–¥–µ–ª—ñ: –∑–∞—Å—Ç–æ—Å—É–π—Ç–µ transfer learning –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤.\n",
    "–ü—Ä–∏–¥—ñ–ª—ñ—Ç—å —É–≤–∞–≥—É –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –æ–±—Ä–æ–±—Ü—ñ –¥–∞–Ω–∏—Ö: –æ—á–∏—Å—Ç—ñ—Ç—å —Ç–µ–∫—Å—Ç, –Ω–æ—Ä–º–∞–ª—ñ–∑—É–π—Ç–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è.\n",
    "\n",
    "–†–µ–≥—É–ª—è—Ä–Ω–æ –ø–µ—Ä–µ–≤—ñ—Ä—è–π—Ç–µ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª—ñ –Ω–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ–º—É –Ω–∞–±–æ—Ä—ñ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ –æ—Ñ—ñ—Ü—ñ–π–Ω—É –º–µ—Ç—Ä–∏–∫—É –∑–º–∞–≥–∞–Ω–Ω—è ‚Äî ‚Äúquadratic weighted kappa‚Äù.\n",
    "\n",
    "–î–æ–¥–∞–π—Ç–µ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü—ñ—é –¥–æ —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç, —â–æ–± –∑–∞–ø–æ–±—ñ–≥—Ç–∏ –ø–µ—Ä–µ–Ω–∞–≤—á–∞–Ω–Ω—é.\n",
    "\n",
    "–í–∏–≤—á–∞–π—Ç–µ —Ä—ñ—à–µ–Ω–Ω—è —É—á–∞—Å–Ω–∏–∫—ñ–≤ –∞–Ω–∞–ª–æ–≥—ñ—á–Ω–∏—Ö –∑–º–∞–≥–∞–Ω—å: –∞–Ω–∞–ª—ñ–∑—É–π—Ç–µ –ø—É–±–ª—ñ—á–Ω—ñ –Ω–æ—É—Ç–±—É–∫–∏ –¥–ª—è –Ω–æ–≤–∏—Ö —ñ–¥–µ–π.\n",
    "\n",
    "–û–ø—Ç–∏–º—ñ–∑—É–π—Ç–µ –≥—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏, –Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫—É Optuna.\n",
    "\n",
    "–ó–≤–µ—Ä–Ω—ñ—Ç—å —É–≤–∞–≥—É –Ω–∞ –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—ñ–≤ ‚Äî –∑–∞—Å—Ç–æ—Å—É–π—Ç–µ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω—ñ –º–µ—Ç–æ–¥–∏, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ.\n",
    "\n",
    "–ù–µ –∑–∞–±—É–≤–∞–π—Ç–µ –ø—Ä–æ —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—é –º–æ–¥–µ–ª—ñ: —Å–ø—Ä–æ–±—É–π—Ç–µ –∑—Ä–æ–∑—É–º—ñ—Ç–∏, —è–∫—ñ —Ñ–∞–∫—Ç–æ—Ä–∏ –Ω–∞–π–±—ñ–ª—å—à–µ –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ –ø—Ä–æ–≥–Ω–æ–∑.\n",
    "\n",
    "–ü–µ—Ä–µ–¥ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è–º –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ–≥–æ –Ω–∞–±–æ—Ä—É (–ø–µ—Ä–µ-) –Ω–∞–≤—á—ñ—Ç—å –≤–∞—à—É —Ñ—ñ–Ω–∞–ª—å–Ω—É –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å—ñ—Ö –¥–æ—Å—Ç—É–ø–Ω–∏—Ö –¥–∞–Ω–∏—Ö (—Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∞ + —Ç–µ—Å—Ç–æ–≤–∞ –≤–∏–±—ñ—Ä–∫–∏).\n",
    "\n",
    "–°—Ñ–æ—Ä–º—É–π—Ç–µ —Ñ–∞–π–ª .csv –∑ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–æ–º —Ç–≤–∞—Ä–∏–Ω–∏ —Ç–∞ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤–∞–Ω–æ—é —à–≤–∏–¥–∫—ñ—Å—Ç—é –∞–¥–æ–ø—Ü—ñ—ó. –ü–æ—Ä—è–¥–æ–∫ —Ä—è–¥–∫—ñ–≤ –Ω–µ –º–∞—î –∑–Ω–∞—á–µ–Ω–Ω—è.\n",
    "\n",
    "–ü—ñ—Å–ª—è –Ω–∞–¥–∞–Ω–Ω—è —Ñ–∞–π–ª—É –∑ –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏ —Ç–∞ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É –º–µ—Ç—Ä–∏–∫–∏ –∑–º–∞–≥–∞–Ω–Ω—è –ø–æ–¥—É–º–∞–π—Ç–µ –ø—Ä–æ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—å –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ —ñ –ø—Ä–æ–¥–æ–≤–∂—É–π—Ç–µ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—É–≤–∞—Ç–∏ –Ω–∞–¥ –ø—ñ–¥–≤–∏—â–µ–Ω–Ω—è–º –º–µ—Ç—Ä–∏–∫–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225afc1",
   "metadata": {},
   "source": [
    "Dataset Description\n",
    "–ù–∞–±—ñ—Ä –¥–∞–Ω–∏—Ö –¥–ª—è –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —ñ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –º—ñ—Å—Ç–∏—Ç—å 6 432 –æ–±'—î–∫—Ç–∏.\n",
    "\n",
    "–í—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ –¥–∞–Ω–æ—ó –∑–∞–¥–∞—á—ñ –º–∞—é—Ç—å –Ω–∞—Å—Ç—É–ø–Ω—ñ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:\n",
    "\n",
    "–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è:\n",
    "–§–æ—Ç–æ–≥—Ä–∞—Ñ—ñ—ó —Ç–≤–∞—Ä–∏–Ω (—Å–æ–±–∞–∫ —Ç–∞ –∫–æ—Ç—ñ–≤).\n",
    "–ú–æ–∂—É—Ç—å –±—É—Ç–∏ —è–∫ –æ–¥–∏–Ω–∏—á–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è, —Ç–∞–∫ —ñ –¥–µ–∫—ñ–ª—å–∫–∞ –∑–æ–±—Ä–∞–∂–µ–Ω—å –¥–ª—è –æ–¥–Ω—ñ—î—ó —Ç–≤–∞—Ä–∏–Ω–∏.\n",
    "–¢–µ–∫—Å—Ç–æ–≤—ñ –¥–∞–Ω—ñ:\n",
    "–û–ø–∏—Å —Ç–≤–∞—Ä–∏–Ω–∏: —Ç–µ–∫—Å—Ç–æ–≤–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è, —è–∫–∞ –æ–ø–∏—Å—É—î —Ö–∞—Ä–∞–∫—Ç–µ—Ä, —ñ—Å—Ç–æ—Ä—ñ—é —Ç–∞ –æ—Å–æ–±–ª–∏–≤–æ—Å—Ç—ñ —Ç–≤–∞—Ä–∏–Ω–∏.\n",
    "–ú–æ–∂–ª–∏–≤–æ, –≤–∫–ª—é—á–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –ø–æ—Ä–æ–¥—É, –≤—ñ–∫, —Å—Ç–∞–Ω –∑–¥–æ—Ä–æ–≤'—è —Ç–æ—â–æ.\n",
    "–¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞:\n",
    "–ö–∞—Ç–µ–≥–æ—Ä—ñ—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ —É—Å–∏–Ω–æ–≤–ª–µ–Ω–Ω—è (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –≤—ñ–¥ 1 –¥–æ 4, –¥–µ 1 ‚Äî –Ω–∞–π—à–≤–∏–¥—à–µ —É—Å–∏–Ω–æ–≤–ª–µ–Ω–Ω—è, 4 ‚Äî –Ω–∞–π–ø–æ–≤—ñ–ª—å–Ω—ñ—à–µ)\n",
    "–í–∞—à–µ –∑–∞–≤–¥–∞–Ω–Ω—è ‚Äî –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —Ü—ñ –¥–∞–Ω—ñ –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ, —è–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞—Ç–∏–º–µ —à–≤–∏–¥–∫—ñ—Å—Ç—å —É—Å–∏–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–≤–∞—Ä–∏–Ω–∏.\n",
    "\n",
    "–î–ª—è —Ä–æ–±–æ—Ç–∏ –≤–∞–º –∑–Ω–∞–¥–æ–±–ª—è—Ç—å—Å—è –Ω–∞—Å—Ç—É–ø–Ω—ñ —Ñ–∞–π–ª–∏, —è–∫—ñ –≤–∏ –∑–Ω–∞–π–¥–µ—Ç–µ –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ –∑–º–∞–≥–∞–Ω–Ω—è:\n",
    "\n",
    "train.csv - –Ω–∞–≤—á–∞–ª—å–Ω–∏–π –Ω–∞–±—ñ—Ä\n",
    "test.csv - –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–∏–π –Ω–∞–±—ñ—Ä\n",
    "sample_submission.csv - —Ñ–∞–π–ª-–∑—Ä–∞–∑–æ–∫ –¥–ª—è –ø–æ–¥–∞–Ω–Ω—è –ø—Ä–æ–≥–Ω–æ–∑—ñ–≤ —É –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b3fab",
   "metadata": {},
   "source": [
    "## **Import the required libraries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af6e03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from seaborn) (2.1.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from seaborn) (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aae8019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "# filter warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b958dcb",
   "metadata": {},
   "source": [
    "\n",
    "## 1. **–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è —Ç–µ–∫—Å—Ç—ñ–≤:**\n",
    "\n",
    "## **Data loading:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dc91b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6431, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>Description</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d3b4f29f8</td>\n",
       "      <td>Mayleen and Flo are two lovely adorable sister...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e9dc82251</td>\n",
       "      <td>A total of 5 beautiful Tabbys available for ad...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8111f6d4a</td>\n",
       "      <td>Two-and-a-half month old girl. Very manja and ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>693a90fda</td>\n",
       "      <td>Neil is a healthy and active ~2-month-old fema...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9d08c85ef</td>\n",
       "      <td>Gray kitten available for adoption in sungai p...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID                                        Description  AdoptionSpeed\n",
       "0  d3b4f29f8  Mayleen and Flo are two lovely adorable sister...              2\n",
       "1  e9dc82251  A total of 5 beautiful Tabbys available for ad...              2\n",
       "2  8111f6d4a  Two-and-a-half month old girl. Very manja and ...              2\n",
       "3  693a90fda  Neil is a healthy and active ~2-month-old fema...              2\n",
       "4  9d08c85ef  Gray kitten available for adoption in sungai p...              2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test  shape: (1891, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6697a7f62</td>\n",
       "      <td>This cute little puppy is looking for a loving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23b64fe21</td>\n",
       "      <td>These 3 puppies was rescued from a mechanic sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41e824cbe</td>\n",
       "      <td>Ara needs a forever home! Believe me, he's a r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6c3d7237b</td>\n",
       "      <td>i rescue this homeless dog 2 years ago but my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97b0b5d92</td>\n",
       "      <td>We found him at a shopping mall at a very clea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID                                        Description\n",
       "0  6697a7f62  This cute little puppy is looking for a loving...\n",
       "1  23b64fe21  These 3 puppies was rescued from a mechanic sh...\n",
       "2  41e824cbe  Ara needs a forever home! Believe me, he's a r...\n",
       "3  6c3d7237b  i rescue this homeless dog 2 years ago but my ...\n",
       "4  97b0b5d92  We found him at a shopping mall at a very clea..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6697a7f62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23b64fe21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41e824cbe</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6c3d7237b</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID  AdoptionSpeed\n",
       "0  6697a7f62              1\n",
       "1  23b64fe21              2\n",
       "2  41e824cbe              3\n",
       "3  6c3d7237b              4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_DIR = \"./PetFinderDatasets\"\n",
    "\n",
    "train_path = os.path.join(DATA_DIR, \"train.csv\")\n",
    "test_path  = os.path.join(DATA_DIR, \"test.csv\")\n",
    "sample_sub = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test  = pd.read_csv(test_path)\n",
    "sample = pd.read_csv(sample_sub)\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "display(train.head())\n",
    "print(\"Test  shape:\", test.shape)\n",
    "display(test.head())\n",
    "print(\"Sample head:\")\n",
    "display(sample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1e314",
   "metadata": {},
   "source": [
    "## **Data checking:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dbb3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–∏–∫–ª–∞–¥–∏ –æ–ø–∏—Å—ñ–≤ —É train:\n",
      "0    Mayleen and Flo are two lovely adorable sister...\n",
      "1    A total of 5 beautiful Tabbys available for ad...\n",
      "2    Two-and-a-half month old girl. Very manja and ...\n",
      "3    Neil is a healthy and active ~2-month-old fema...\n",
      "4    Gray kitten available for adoption in sungai p...\n",
      "Name: Description, dtype: object\n",
      "\n",
      "Class distribution (AdoptionSpeed) in train:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdoptionSpeed\n",
       "1    1197\n",
       "2    1773\n",
       "3    1328\n",
       "4    2133\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# –±–∞–∑–æ–≤–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "need_train_cols = [\"PetID\", \"AdoptionSpeed\", \"Description\"]\n",
    "need_test_cols  = [\"PetID\", \"Description\"]\n",
    "\n",
    "for c in need_train_cols:\n",
    "    assert c in train.columns, f\"–í train.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–æ–ª–æ–Ω–∫—É {c}\"\n",
    "for c in need_test_cols:\n",
    "    assert c in test.columns, f\"–í test.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∫–æ–ª–æ–Ω–∫—É {c}\"\n",
    "\n",
    "train[\"Description\"] = train[\"Description\"].fillna(\"\") # –∑–∞–º—ñ–Ω—é—î–º–æ NaN –Ω–∞ –ø—É—Å—Ç–∏–π —Ä—è–¥–æ–∫, —â–æ–± –Ω–µ –±—É–ª–æ –∑–±–æ—ó–≤ —É TF-IDF.\n",
    "print(\"–ü—Ä–∏–∫–ª–∞–¥–∏ –æ–ø–∏—Å—ñ–≤ —É train:\")\n",
    "print(train[\"Description\"].head(5))\n",
    "\n",
    "test[\"Description\"]  = test[\"Description\"].fillna(\"\")\n",
    "\n",
    "print(\"\\nClass distribution (AdoptionSpeed) in train:\")\n",
    "display(train[\"AdoptionSpeed\"].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398bad2",
   "metadata": {},
   "source": [
    "## **Data preparation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604cddee",
   "metadata": {},
   "source": [
    "Train/Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e17288a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split size: 5144\n",
      "Val split size:   1287\n",
      "Target distrib train:\n",
      "AdoptionSpeed\n",
      "1    0.186236\n",
      "2    0.275661\n",
      "3    0.206454\n",
      "4    0.331649\n",
      "Name: proportion, dtype: float64\n",
      "Target distrib val:\n",
      "AdoptionSpeed\n",
      "1    0.185703\n",
      "2    0.275835\n",
      "3    0.206682\n",
      "4    0.331779\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    train[\"Description\"],\n",
    "    train[\"AdoptionSpeed\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train[\"AdoptionSpeed\"]\n",
    ")\n",
    "\n",
    "print(\"Train split size:\", X_tr.shape[0])\n",
    "print(\"Val split size:  \", X_val.shape[0])\n",
    "print(\"Target distrib train:\")\n",
    "print(y_tr.value_counts(normalize=True).sort_index())\n",
    "print(\"Target distrib val:\")\n",
    "print(y_val.value_counts(normalize=True).sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16d1355",
   "metadata": {},
   "source": [
    "## **–ü–æ–±—É–¥–æ–≤–∞ –ø–∞–π–ø–ª–∞–π–Ω—É TF-IDF + LogisticRegression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3d69927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK (word+char): 0.8324351768853542\n"
     ]
    }
   ],
   "source": [
    "ct = ColumnTransformer([\n",
    "    (\"w\", TfidfVectorizer(max_features=10000, ngram_range=(1,2),\n",
    "                          min_df=3, sublinear_tf=True,\n",
    "                          lowercase=True, strip_accents=\"unicode\"), \"Description\"),\n",
    "    (\"c\", TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5),\n",
    "                          min_df=3, sublinear_tf=True), \"Description\")\n",
    "])\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_char = Pipeline([\n",
    "    (\"feats\", ct),\n",
    "    (\"clf\", LogisticRegression(max_iter=300, C=1.0, solver=\"lbfgs\",\n",
    "                               multi_class=\"multinomial\", class_weight=None,\n",
    "                               random_state=42))\n",
    "])\n",
    "\n",
    "pipe_char.fit(train[[\"Description\"]], train[\"AdoptionSpeed\"])\n",
    "pred = pipe_char.predict(pd.DataFrame({\"Description\": X_val}))\n",
    "print(\"QWK (word+char):\", cohen_kappa_score(y_val, pred, weights=\"quadratic\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c57d4",
   "metadata": {},
   "source": [
    "## **–ù–∞–≤—á–∞–Ω–Ω—è —ñ –ª–æ–∫–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞ (QWK):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c728d74",
   "metadata": {},
   "source": [
    "## **–§—ñ–Ω–∞–ª—å–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –Ω–∞ –≤—Å—å–æ–º—É train + submission.csv:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef49b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_char.fit(train[[\"Description\"]], train[\"AdoptionSpeed\"])\n",
    "preds = pipe_char.predict(test[[\"Description\"]])\n",
    "submission = pd.DataFrame({\"PetID\": test[\"PetID\"], \"AdoptionSpeed\": preds})\n",
    "submission.to_csv(\"submission_text_only.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fc6cb",
   "metadata": {},
   "source": [
    "## 2. **–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –∫–∞—Ä—Ç–∏–Ω–æ–∫:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb0c2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.autoaugment import AutoAugment, AutoAugmentPolicy\n",
    "# —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó –¥–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω—å\n",
    "img_tfms_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((224, 224), scale=(0.7, 1.0), ratio=(0.75, 1.333)),\n",
    "    AutoAugment(policy=AutoAugmentPolicy.IMAGENET),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "img_tfms_eval = transforms.Compose([\n",
    "    transforms.Resize((240, 240)),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46f08f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Utility: find image path for given PetID (search recursively) ===\n",
    "def find_image(img_dir, petid):\n",
    "    # search in root and subfolders, any extension\n",
    "    pats = [\n",
    "        os.path.join(img_dir, f\"{petid}*.*\"),\n",
    "        os.path.join(img_dir, \"**\", f\"{petid}*.*\"),\n",
    "    ]\n",
    "    for pat in pats:\n",
    "        hits = glob.glob(pat, recursive=True)\n",
    "        hits = [h for h in hits if h.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "        if hits:\n",
    "            return sorted(hits)[0]\n",
    "    print(\"None\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ece9e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>Description</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d3b4f29f8</td>\n",
       "      <td>Mayleen and Flo are two lovely adorable sister...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e9dc82251</td>\n",
       "      <td>A total of 5 beautiful Tabbys available for ad...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8111f6d4a</td>\n",
       "      <td>Two-and-a-half month old girl. Very manja and ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>693a90fda</td>\n",
       "      <td>Neil is a healthy and active ~2-month-old fema...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9d08c85ef</td>\n",
       "      <td>Gray kitten available for adoption in sungai p...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID                                        Description  AdoptionSpeed\n",
       "0  d3b4f29f8  Mayleen and Flo are two lovely adorable sister...              2\n",
       "1  e9dc82251  A total of 5 beautiful Tabbys available for ad...              2\n",
       "2  8111f6d4a  Two-and-a-half month old girl. Very manja and ...              2\n",
       "3  693a90fda  Neil is a healthy and active ~2-month-old fema...              2\n",
       "4  9d08c85ef  Gray kitten available for adoption in sungai p...              2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f28625d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6431, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a6fcd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "Missing train images: 0\n",
      "Missing test  images: 4\n"
     ]
    }
   ],
   "source": [
    "# Pre-compute image paths\n",
    "\n",
    "TRAIN_IMG_DIR = 'PetFinderDatasets/images/images/train'\n",
    "TEST_IMG_DIR = 'PetFinderDatasets/images/images/test'\n",
    "\n",
    "test[\"img_path\"]  = test[\"PetID\"].apply(lambda x: find_image(TEST_IMG_DIR,  x))\n",
    "\n",
    "train[\"img_path\"] = train[\"PetID\"].apply(lambda x: find_image(TRAIN_IMG_DIR, x))\n",
    "print(\"Missing train images:\", train[\"img_path\"].isna().sum())\n",
    "\n",
    "print(\"Missing test  images:\", test[\"img_path\"].isna().sum())\n",
    "# TEST: keep ALL rows (submission must include every PetID)\n",
    "# We'll use a placeholder tensor when image is missing/corrupted.\n",
    "placeholder = torch.zeros(3, 224, 224)  # black square tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c97f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels {1,2,3,4} ‚Üí indices {0,1,2,3}\n",
    "classes_sorted = sorted(train[\"AdoptionSpeed\"].unique())  # e.g. [1,2,3,4]\n",
    "label2idx = {lab: i for i, lab in enumerate(classes_sorted)}\n",
    "idx2label = {i: lab for i, lab in enumerate(classes_sorted)}\n",
    "num_classes = len(classes_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92f73f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PetDataset(Dataset):\n",
    "    def __init__(self, df, training=True, tfms_train=None, tfms_eval=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.training = training\n",
    "        self.tfms_train = tfms_train\n",
    "        self.tfms_eval  = tfms_eval\n",
    "        self.has_labels = \"AdoptionSpeed\" in self.df.columns\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        img_path = row[\"img_path\"]\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            tfm = self.tfms_train if self.training else self.tfms_eval\n",
    "            x = tfm(img)\n",
    "        except:\n",
    "            x = placeholder.clone()\n",
    "\n",
    "        if self.has_labels:\n",
    "            # for val and train\n",
    "            y = label2idx[int(row[\"AdoptionSpeed\"])]\n",
    "            return x, y\n",
    "        else:\n",
    "            # for test\n",
    "            return x, row[\"PetID\"]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58919144",
   "metadata": {},
   "source": [
    "## **Train/Val split + DataLoader-–∏:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57dd5e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 161\n",
      "Val batches:   41\n",
      "Test batches:  60\n"
     ]
    }
   ],
   "source": [
    "# train/val split\n",
    "tr_df, val_df = train_test_split(\n",
    "    train, test_size=0.2, random_state=42, stratify=train[\"AdoptionSpeed\"]\n",
    ")\n",
    "\n",
    "# —Å—Ç–≤–æ—Ä—é—î–º–æ –¥–∞—Ç–∞—Å–µ—Ç–∏\n",
    "tr_ds  = PetDataset(tr_df,  training=True,  tfms_train=img_tfms_train, tfms_eval=img_tfms_eval)\n",
    "val_ds = PetDataset(val_df,  training=False, tfms_train=img_tfms_train, tfms_eval=img_tfms_eval)  # eval tfms\n",
    "te_ds  = PetDataset(test,    training=False, tfms_train=img_tfms_train, tfms_eval=img_tfms_eval)  # eval tfms, returns (x, PetID)\n",
    "\n",
    "# DataLoader-–∏\n",
    "tr_dl  = DataLoader(tr_ds, batch_size=32, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
    "te_dl  = DataLoader(te_ds,  batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"Train batches:\", len(tr_dl))\n",
    "print(\"Val batches:  \", len(val_dl))\n",
    "print(\"Test batches: \", len(te_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0901d467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "tensor([2, 0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in tr_dl:\n",
    "    print(images.shape)   # (batch_size, 3, 224, 224)\n",
    "    print(labels[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62552438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "tensor([2, 0, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in val_dl:\n",
    "    print(images.shape)   # (batch_size, 3, 224, 224)\n",
    "    print(labels[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45cac0f",
   "metadata": {},
   "source": [
    "## **–ú–æ–¥–µ–ª—å ResNet18 (Transfer Learning):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52a42de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Dropout(p=0.25, inplace=False)\n",
      "  (1): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# —Å—Ç–≤–æ—Ä—é—î–º–æ ResNet18 –∑ pretrained –≤–∞–≥–∞–º–∏\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# –∑–∞–º—ñ–Ω—é—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —à–∞—Ä (fc) –Ω–∞ 4 –∫–ª–∞—Å–∏\n",
    "in_feats = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(p=0.25), \n",
    "    nn.Linear(in_feats, num_classes)\n",
    ")\n",
    "print(model.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loss + Optimizer + GPU:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22f05e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# —Ñ—É–Ω–∫—Ü—ñ—è –≤—Ç—Ä–∞—Ç (–∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# –æ–ø—Ç–∏–º—ñ–∑–∞—Ç–æ—Ä (Adam)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8803e4c9",
   "metadata": {},
   "source": [
    "## **–¶–∏–∫–ª –Ω–∞–≤—á–∞–Ω–Ω—è (train/val) + –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è best-–º–æ–¥–µ–ª—ñ:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95841e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Train loss 1.3915 acc 0.324 | Val loss 1.4104 acc 0.353\n",
      "Epoch 02 | Train loss 1.3877 acc 0.324 | Val loss 1.3517 acc 0.334\n",
      "Epoch 03 | Train loss 1.3691 acc 0.325 | Val loss 1.3927 acc 0.274\n",
      "Epoch 04 | Train loss 1.3659 acc 0.329 | Val loss 1.7757 acc 0.322\n",
      "Epoch 05 | Train loss 1.3747 acc 0.321 | Val loss 1.3515 acc 0.340\n",
      "Epoch 06 | Train loss 1.3619 acc 0.332 | Val loss 1.3850 acc 0.318\n",
      "Epoch 07 | Train loss 1.3682 acc 0.335 | Val loss 1.7715 acc 0.330\n",
      "Epoch 08 | Train loss 1.3690 acc 0.326 | Val loss 1.3541 acc 0.342\n",
      "Epoch 09 | Train loss 1.3584 acc 0.343 | Val loss 1.4055 acc 0.350\n",
      "Epoch 10 | Train loss 1.3589 acc 0.337 | Val loss 1.3780 acc 0.338\n",
      "Epoch 11 | Train loss 1.3582 acc 0.337 | Val loss 1.3692 acc 0.329\n",
      "Epoch 12 | Train loss 1.3541 acc 0.349 | Val loss 1.3586 acc 0.336\n",
      "Epoch 13 | Train loss 1.3566 acc 0.334 | Val loss 1.3636 acc 0.343\n",
      "Epoch 14 | Train loss 1.3558 acc 0.343 | Val loss 1.3486 acc 0.345\n",
      "Epoch 15 | Train loss 1.3559 acc 0.343 | Val loss 1.3583 acc 0.340\n",
      "Epoch 16 | Train loss 1.3562 acc 0.338 | Val loss 1.3455 acc 0.362\n",
      "Epoch 17 | Train loss 1.3554 acc 0.336 | Val loss 1.4482 acc 0.332\n",
      "Epoch 18 | Train loss 1.3580 acc 0.340 | Val loss 1.7898 acc 0.340\n",
      "Epoch 19 | Train loss 1.3524 acc 0.342 | Val loss 1.3619 acc 0.345\n",
      "Epoch 20 | Train loss 1.3530 acc 0.340 | Val loss 1.3512 acc 0.346\n",
      "Epoch 21 | Train loss 1.3538 acc 0.345 | Val loss 1.3545 acc 0.359\n",
      "Epoch 22 | Train loss 1.3510 acc 0.341 | Val loss 1.3637 acc 0.340\n",
      "Epoch 23 | Train loss 1.3523 acc 0.340 | Val loss 1.3679 acc 0.357\n",
      "Epoch 24 | Train loss 1.3534 acc 0.339 | Val loss 1.3498 acc 0.341\n",
      "Epoch 25 | Train loss 1.3498 acc 0.349 | Val loss 1.3518 acc 0.355\n",
      "Epoch 26 | Train loss 1.3519 acc 0.341 | Val loss 1.6361 acc 0.354\n",
      "Epoch 27 | Train loss 1.3516 acc 0.344 | Val loss 1.3489 acc 0.348\n",
      "Epoch 28 | Train loss 1.3485 acc 0.347 | Val loss 1.3521 acc 0.348\n",
      "Epoch 29 | Train loss 1.3480 acc 0.347 | Val loss 1.3803 acc 0.311\n",
      "Epoch 30 | Train loss 1.3506 acc 0.351 | Val loss 1.3503 acc 0.351\n",
      "Epoch 31 | Train loss 1.3493 acc 0.348 | Val loss 1.3489 acc 0.359\n",
      "Epoch 32 | Train loss 1.3502 acc 0.343 | Val loss 1.3490 acc 0.354\n",
      "Epoch 33 | Train loss 1.3458 acc 0.351 | Val loss 1.3651 acc 0.343\n",
      "Epoch 34 | Train loss 1.3475 acc 0.348 | Val loss 1.3442 acc 0.360\n",
      "Epoch 35 | Train loss 1.3471 acc 0.344 | Val loss 1.3556 acc 0.335\n",
      "Epoch 36 | Train loss 1.3449 acc 0.343 | Val loss 1.3570 acc 0.332\n",
      "Epoch 37 | Train loss 1.3464 acc 0.346 | Val loss 1.3608 acc 0.351\n",
      "Epoch 38 | Train loss 1.3430 acc 0.349 | Val loss 1.4105 acc 0.349\n",
      "Epoch 39 | Train loss 1.3413 acc 0.345 | Val loss 1.3497 acc 0.355\n",
      "Epoch 40 | Train loss 1.3396 acc 0.354 | Val loss 1.3531 acc 0.340\n",
      "Epoch 41 | Train loss 1.3404 acc 0.357 | Val loss 1.3527 acc 0.322\n",
      "Epoch 42 | Train loss 1.3408 acc 0.357 | Val loss 1.3506 acc 0.341\n",
      "Epoch 43 | Train loss 1.3409 acc 0.350 | Val loss 1.3626 acc 0.341\n",
      "Epoch 44 | Train loss 1.3405 acc 0.353 | Val loss 1.3499 acc 0.342\n",
      "Epoch 45 | Train loss 1.3388 acc 0.359 | Val loss 1.3523 acc 0.349\n",
      "Epoch 46 | Train loss 1.3397 acc 0.356 | Val loss 1.3493 acc 0.348\n",
      "Epoch 47 | Train loss 1.3393 acc 0.359 | Val loss 1.3403 acc 0.364\n",
      "Epoch 48 | Train loss 1.3351 acc 0.363 | Val loss 1.3610 acc 0.342\n",
      "Epoch 49 | Train loss 1.3393 acc 0.353 | Val loss 1.3379 acc 0.366\n",
      "Epoch 50 | Train loss 1.3352 acc 0.358 | Val loss 1.3543 acc 0.361\n",
      "‚úÖ Training finished. Best val acc: 0.36596736596736595\n"
     ]
    }
   ],
   "source": [
    "#from tqdm.auto import tqdm # –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä\n",
    "\n",
    "epochs = 50   # baseline, –º–æ–∂–Ω–∞ –∑–±—ñ–ª—å—à–∏—Ç–∏\n",
    "best_val_acc = 0.0\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # ----- TRAIN -----\n",
    "    model.train()\n",
    "    train_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    #train_loader = tqdm(tr_dl, desc=f\"Epoch {epoch:02d} [Train]\")\n",
    "\n",
    "    for xb, yb in tr_dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "\n",
    "        #train_loader.set_postfix(loss=f\"{(train_loss/total):.4f}\", acc=f\"{(correct/total):.3f}\")\n",
    "    \n",
    "    train_loss /= total\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    # ----- VALIDATION -----\n",
    "    model.eval()\n",
    "\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    #val_loader = tqdm(val_dl, desc=f\"Epoch {epoch:02d} [Val]\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_dl:\n",
    "            \n",
    "            xb, yb = xb.to(device), yb.to(device)        \n",
    " \n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            \n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "            #val_loader.set_postfix(loss=f\"{(val_loss/total):.4f}\", acc=f\"{(correct/total):.3f}\")\n",
    "\n",
    "    val_loss /= total\n",
    "    val_acc = correct / total\n",
    "    \n",
    "    # –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_resnet18.pth\")\n",
    "    \n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch:02d} | Train loss {train_loss:.4f} acc {train_acc:.3f} \"\n",
    "          f\"| Val loss {val_loss:.4f} acc {val_acc:.3f}\")\n",
    "\n",
    "print(\"‚úÖ Training finished. Best val acc:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2229e",
   "metadata": {},
   "source": [
    "## **–Ü–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ test + submission.csv:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90d3e893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best weights: best_resnet18.pth\n",
      "‚úÖ Saved: submission_images_only.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PetID</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6697a7f62</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23b64fe21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41e824cbe</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6c3d7237b</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97b0b5d92</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PetID  AdoptionSpeed\n",
       "0  6697a7f62              4\n",
       "1  23b64fe21              2\n",
       "2  41e824cbe              4\n",
       "3  6c3d7237b              4\n",
       "4  97b0b5d92              4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# –ø—ñ–¥–≤–∞–Ω—Ç–∞–∂–∏–º–æ best-–≤–∞–≥–∏, —è–∫—â–æ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ –Ω–∞ –∫—Ä–æ—Ü—ñ 5\n",
    "if os.path.exists(\"best_resnet18.pth\"):\n",
    "    model.load_state_dict(torch.load(\"best_resnet18.pth\", map_location=device))\n",
    "    print(\"Loaded best weights: best_resnet18.pth\")\n",
    "model.eval()\n",
    "\n",
    "# –º–∞–ø–∞ —ñ–Ω–¥–µ–∫—Å—ñ–≤ —É –≤–∏—Ö—ñ–¥–Ω—ñ —è—Ä–ª–∏–∫–∏ (–ø—ñ–¥–ª–∞—à—Ç–æ–≤—É—î–º–æ—Å—å –ø—ñ–¥ –≤–∞—à train: —á–∏ —Ü–µ [0..3], —á–∏ [1..4])\n",
    "classes_sorted = sorted(train[\"AdoptionSpeed\"].unique())   # –Ω–∞–ø—Ä., [1,2,3,4] –∞–±–æ [0,1,2,3]\n",
    "idx2label = {i: classes_sorted[i] for i in range(4)}\n",
    "\n",
    "preds, ids = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, pet_ids in te_dl:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        pred_idx = out.argmax(1).cpu().numpy()\n",
    "        pred_lab = [idx2label[int(i)] for i in pred_idx]\n",
    "        preds.extend(pred_lab)\n",
    "        ids.extend(pet_ids)\n",
    "\n",
    "submission = pd.DataFrame({\"PetID\": ids, \"AdoptionSpeed\": np.array(preds, dtype=int)})\n",
    "submission.to_csv(\"submission_images_only.csv\", index=False)\n",
    "print(\"‚úÖ Saved:\", \"submission_images_only.csv\")\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f67ed0c",
   "metadata": {},
   "source": [
    "## 3. **–ê–Ω—Å–∞–º–±–ª—å (—Ç–µ–∫—Å—Ç + –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b8aa13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–ø—ñ—è —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö —ñ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ PetID\n",
    "test = test.copy()\n",
    "test[\"PetID\"] = test[\"PetID\"].astype(str).str.strip()\n",
    "X_test_desc = test[[\"Description\"]].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5544cd",
   "metadata": {},
   "source": [
    "*–†–æ–∑—Ä–∞–∑–æ–≤—É—î–º–æ —è–∫—ñ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—é.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5bf292ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK text: 0.6348906518987774\n",
      "QWK img: 0.17340372675825344\n",
      "Best ensemble QWK: 0.63625 at alpha=0.80\n"
     ]
    }
   ],
   "source": [
    "qwk_text = cohen_kappa_score(y_val, pred_text_val, weights=\"quadratic\")\n",
    "print(\"QWK text:\", qwk_text)\n",
    "\n",
    "# 2) Image probabilities on validation\n",
    "proba_img_list, ids_val = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for xb, pet_ids in val_dl:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        probs = torch.softmax(out, dim=1).cpu().numpy()\n",
    "        proba_img_list.append(probs)\n",
    "        ids_val.extend(pet_ids)\n",
    "\n",
    "proba_img_val = np.vstack(proba_img_list)\n",
    "pred_img_val = proba_img_val.argmax(axis=1)\n",
    "\n",
    "qwk_img = cohen_kappa_score(y_val, pred_img_val, weights=\"quadratic\")\n",
    "print(\"QWK img:\", qwk_img)\n",
    "\n",
    "# 3) Ensemble sweep over alpha\n",
    "alphas = np.linspace(0.0, 1.0, 11)\n",
    "best_alpha, best_qwk = None, -1\n",
    "for a in alphas:\n",
    "    p = a * proba_text_val + (1.0 - a) * proba_img_val\n",
    "    pred = p.argmax(axis=1)\n",
    "    q = cohen_kappa_score(y_val, pred, weights=\"quadratic\")\n",
    "    if q > best_qwk:\n",
    "        best_qwk, best_alpha = q, a\n",
    "\n",
    "print(f\"Best ensemble QWK: {best_qwk:.5f} at alpha={best_alpha:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9b8311",
   "metadata": {},
   "source": [
    "*–ì–æ—Ç—É—î–º–æ —Å–∞–±–º—ñ—à–∏–Ω*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e0517c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ô–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –∑ —Ç–µ–∫—Å—Ç–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ\n",
    "proba_text = pipe_char.predict_proba(X_test_desc)\n",
    "classes_text = pipe_char.named_steps[\"clf\"].classes_\n",
    "text_cols = [f\"text_{c}\" for c in classes_text]\n",
    "proba_text_df = pd.DataFrame(proba_text, columns=text_cols)\n",
    "proba_text_df[\"PetID\"] = test[\"PetID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "879c2a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ –º–æ–∂–Ω–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –∑–æ–±—Ä–∞–∂–µ–Ω—å\n",
    "use_images = (\"proba_img\" in locals()) and (\"ids\" in locals()) and (proba_img is not None)\n",
    "if use_images:\n",
    "    classes_img = sorted(train[\"AdoptionSpeed\"].unique().tolist())\n",
    "    if getattr(proba_img, \"shape\", None) is None or proba_img.shape[0] == 0:\n",
    "        use_images = False\n",
    "    elif proba_img.shape[1] != len(classes_img):\n",
    "        if proba_img.shape[1] == len(classes_text):\n",
    "            classes_img = list(classes_text)\n",
    "        else:\n",
    "            use_images = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "666a8388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§–æ—Ä–º—É—î–º–æ DataFrame –¥–ª—è –∑–æ–±—Ä–∞–∂–µ–Ω—å (—è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω—ñ)\n",
    "if use_images:\n",
    "    img_cols = [f\"img_{c}\" for c in classes_img]\n",
    "    proba_img_df = pd.DataFrame(proba_img, columns=img_cols)\n",
    "    proba_img_df[\"PetID\"] = pd.Series(ids).astype(str).str.strip()\n",
    "else:\n",
    "    proba_img_df = pd.DataFrame({\"PetID\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "deb1313f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–≤–Ω–∏–π —Å–ø–∏—Å–æ–∫ –º—ñ—Ç–æ–∫ –∫–ª–∞—Å—ñ–≤ (—Ç–µ–∫—Å—Ç + –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è)\n",
    "labels_all = sorted(set(classes_text).union(set(classes_img))) if use_images else list(classes_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2233dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–æ–¥–∞—î–º–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ —Å—Ç–æ–≤–ø—Ü—ñ —É —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É/–∑–æ–±—Ä–∞–∂–µ–Ω–Ω–µ–≤–æ–º—É DataFrame\n",
    "for c in labels_all:\n",
    "    tc = f\"text_{c}\"\n",
    "    if tc not in proba_text_df.columns:\n",
    "        proba_text_df[tc] = 0.0\n",
    "if use_images:\n",
    "    for c in labels_all:\n",
    "        ic = f\"img_{c}\"\n",
    "        if ic not in proba_img_df.columns:\n",
    "            proba_img_df[ic] = np.nan\n",
    "\n",
    "# –ó‚Äô—î–¥–Ω—É—î–º–æ —Ç–µ—Å—Ç –∑ —Ç–µ–∫—Å—Ç–æ–≤–∏–º–∏ –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—è–º–∏\n",
    "keep_text = [\"PetID\"] + [f\"text_{c}\" for c in labels_all]\n",
    "merged = test[[\"PetID\"]].merge(proba_text_df[keep_text], on=\"PetID\", how=\"left\")\n",
    "\n",
    "# –Ø–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω—ñ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è ‚Äî –¥–æ–¥–∞—î–º–æ —ó—Ö\n",
    "if use_images:\n",
    "    keep_img = [\"PetID\"] + [f\"img_{c}\" for c in labels_all]\n",
    "    merged = merged.merge(proba_img_df[keep_img], on=\"PetID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f05699f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∞–≥–∞ —Ç–µ–∫—Å—Ç—É —ñ –∑–æ–±—Ä–∞–∂–µ–Ω—å\n",
    "alpha, beta = 0.5, 0.5\n",
    "if use_images:\n",
    "    img_cols_all = [f\"img_{c}\" for c in labels_all]\n",
    "    has_img = merged[img_cols_all].notna().all(axis=1).values\n",
    "    w_text = np.where(has_img, alpha, 1.0)   # —è–∫—â–æ —î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è\n",
    "    w_img  = np.where(has_img, beta,  0.0)   # —è–∫—â–æ –Ω–µ–º–∞—î –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è\n",
    "else:\n",
    "    w_text = np.ones(len(merged), dtype=float)\n",
    "    w_img  = np.zeros(len(merged), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f6efc585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–±—á–∏—Å–ª–µ–Ω–Ω—è –∞–Ω—Å–∞–º–±–ª–µ–≤–∏—Ö –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π\n",
    "for c in labels_all:\n",
    "    text_col = merged[f\"text_{c}\"].fillna(0.0).to_numpy()\n",
    "    img_col  = merged[f\"img_{c}\"].fillna(0.0).to_numpy() if f\"img_{c}\" in merged.columns else np.zeros(len(merged))\n",
    "    merged[f\"p_{c}\"] = w_text * text_col + w_img * img_col\n",
    "\n",
    "# –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π (—â–æ–± —É —Ä—è–¥–∫—É —Å—É–º–∞ = 1)\n",
    "p_cols = [f\"p_{c}\" for c in labels_all]\n",
    "row_sum = merged[p_cols].sum(axis=1).replace(0, 1.0)\n",
    "merged[p_cols] = merged[p_cols].div(row_sum, axis=0)\n",
    "\n",
    "# –§—ñ–Ω–∞–ª—å–Ω–∏–π –∫–ª–∞—Å = argmax –ø–æ –∞–Ω—Å–∞–º–±–ª—é\n",
    "pos = merged[p_cols].values.argmax(axis=1)\n",
    "labels_by_pos = np.array(labels_all)\n",
    "merged[\"AdoptionSpeed\"] = labels_by_pos[pos].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6eab13f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: submission_ensemble_text_img.csv\n"
     ]
    }
   ],
   "source": [
    "# –§–æ—Ä–º—É—î–º–æ —Å–∞–±–º—ñ—à–Ω —É —Ñ–æ—Ä–º–∞—Ç—ñ Kaggle\n",
    "submission = test[[\"PetID\"]].merge(merged[[\"PetID\",\"AdoptionSpeed\"]], on=\"PetID\", how=\"left\")\n",
    "submission.to_csv(\"submission_ensemble_text_img.csv\", index=False)\n",
    "print(\"‚úÖ Saved:\", \"submission_ensemble_text_img.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60cfc73",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# –ó–≤—ñ—Ç –ø–æ –ø—Ä–æ—î–∫—Ç—É Pet Adoption Prediction\n",
    "\n",
    "## –ú–µ—Ç–∞\n",
    "\n",
    "–†–æ–∑—Ä–æ–±–∏—Ç–∏ –º–æ–¥–µ–ª—å –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è —à–≤–∏–¥–∫–æ—Å—Ç—ñ —É—Å–∏–Ω–æ–≤–ª–µ–Ω–Ω—è —Ç–≤–∞—Ä–∏–Ω –Ω–∞ –æ—Å–Ω–æ–≤—ñ –æ–ø–∏—Å—ñ–≤ (—Ç–µ–∫—Å—Ç) —Ç–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ—ñ–π (–∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è). –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∏—Å—å:\n",
    "\n",
    "* **–¢–µ–∫—Å—Ç–∏:** –æ–ø–∏—Å —Ç–≤–∞—Ä–∏–Ω –∑ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö PetFinder.my.\n",
    "* **–§–æ—Ç–æ:** –æ–±—Ä–æ–±–∫–∞ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º ResNet18.\n",
    "* **–ê–Ω—Å–∞–º–±–ª—å:** –ø–æ—î–¥–Ω–∞–Ω–Ω—è –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π –≤—ñ–¥ —Ç–µ–∫—Å—Ç–æ–≤–æ—ó —Ç–∞ –∑–æ–±—Ä–∞–∂–µ–Ω–µ–≤–æ—ó –º–æ–¥–µ–ª–µ–π.\n",
    "\n",
    "## –ü–æ—Ç–æ—á–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏\n",
    "\n",
    "* **–¢–µ–∫—Å—Ç–æ–≤–∞ –º–æ–¥–µ–ª—å (LogisticRegression + TF-IDF):** –ø–æ–∫–∞–∑–∞–ª–∞ —Å—Ç–∞–±—ñ–ª—å–Ω–∏–π –±–∞–∑–æ–≤–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç, –æ—Å–æ–±–ª–∏–≤–æ –∑–∞–≤–¥—è–∫–∏ –ø–æ—î–¥–Ω–∞–Ω–Ω—é word- —Ç–∞ char-n-gram.\n",
    "* **–ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è (ResNet18):** –º–æ–¥–µ–ª—å –ø—Ä–∞—Ü—é—î, –∞–ª–µ —è–∫—ñ—Å—Ç—å –≤–∏—è–≤–∏–ª–∞—Å—å –Ω–∏–∂—á–æ—é –æ—á—ñ–∫—É–≤–∞–Ω–æ—ó —á–µ—Ä–µ–∑ –Ω–µ–¥–æ–ø—Ä–∞—Ü—é–≤–∞–Ω–Ω—è —É –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—ñ–Ω–≥—É —Ñ–æ—Ç–æ (–Ω–µ—Ä—ñ–≤–Ω–æ–º—ñ—Ä–Ω—ñ —Ä–æ–∑–º—ñ—Ä–∏, –≤—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó, —Å–ª–∞–±–∫–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö).\n",
    "* **–ê–Ω—Å–∞–º–±–ª—å (text+image):** –æ—á—ñ–∫—É–≤–∞–Ω–æ–≥–æ –ø—Ä–∏—Ä–æ—Å—Ç—É –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º—É —Ç–µ—Å—Ç—ñ –Ω–µ –¥–∞–ª–æ, —è–∫—ñ—Å—Ç—å –∑–∞–ª–∏—à–∏–ª–∞—Å—è –Ω–∞ —Ä—ñ–≤–Ω—ñ —Ç–µ–∫—Å—Ç–æ–≤–æ—ó –º–æ–¥–µ–ª—ñ.\n",
    "\n",
    "## –ü—Ä–æ–±–ª–µ–º–∏\n",
    "\n",
    "1. **–§–æ—Ç–æ**\n",
    "\n",
    "   * –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—è –æ—á–∏—Å—Ç–∫–∞ —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö.\n",
    "   * –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –æ–¥–Ω—ñ—î—ó –º–æ–¥–µ–ª—ñ (ResNet18) –±–µ–∑ —Ç–æ–Ω–∫–æ–≥–æ —Ç—é–Ω—ñ–Ω–≥—É —á–∏ data augmentation.\n",
    "   * –Ü–≥–Ω–æ—Ä—É–≤–∞–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∑–æ–±—Ä–∞–∂–µ–Ω—å (—á–∞—Å—Ç–∏–Ω–∞ —Ç–≤–∞—Ä–∏–Ω –º–∞—î 1 —Ñ–æ—Ç–æ, —ñ–Ω—à—ñ ‚Äî –¥–æ 5).\n",
    "\n",
    "2. **–¢–µ–∫—Å—Ç–∏**\n",
    "\n",
    "   * –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞–ª–∞—Å—å –ø—Ä–æ—Å—Ç–∞ TF-IDF + Logistic Regression.\n",
    "   * –ù–µ –∑–∞—Å—Ç–æ—Å–æ–≤—É–≤–∞–ª–∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∏ (BERT, mBERT, distilBERT) —á–∏ —Å—É—á–∞—Å–Ω—ñ –µ–º–±–µ–¥–¥–∏–Ω–≥–∏.\n",
    "   * –°–ª–∞–±–∫–µ –≤—Ä–∞—Ö—É–≤–∞–Ω–Ω—è –º–µ—Ç–∞–¥–∞–Ω–∏—Ö (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –¥–æ–≤–∂–∏–Ω–∞ –æ–ø–∏—Å—É, –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–∏–º–≤–æ–ª—ñ–≤).\n",
    "\n",
    "3. **–ê–Ω—Å–∞–º–±–ª—å**\n",
    "\n",
    "   * –§—ñ–∫—Å–æ–≤–∞–Ω—ñ –≤–∞–≥–∏ (0.5/0.5) –¥–ª—è —Ç–µ–∫—Å—Ç—É —Ç–∞ —Ñ–æ—Ç–æ.\n",
    "   * –í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—ñ–¥–±–æ—Ä—É –≤–∞–≥ (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ñ–æ—Ç–æ).\n",
    "   * –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–æ –º–µ—Ç–∞-–º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–º–±—ñ–Ω—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤.\n",
    "\n",
    "## –ü–ª–∞–Ω –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è\n",
    "\n",
    "### 1. –ó–æ–±—Ä–∞–∂–µ–Ω–Ω—è\n",
    "\n",
    "* –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ **ResNet50 / EfficientNet / ViT** –∑ –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–∏–º–∏ –≤–∞–≥–∞–º–∏.\n",
    "* –î–æ–¥–∞—Ç–∏ **data augmentation** (–∫—Ä–æ–ø–∏, —Ñ–ª—ñ–ø–∏, –∫–æ–ª—å–æ—Ä–æ–≤—ñ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü—ñ—ó).\n",
    "* –ù–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ñ–æ—Ç–æ –ø—ñ–¥ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ mean/std ImageNet.\n",
    "* –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ **–∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ–æ—Ç–æ —è–∫ –æ–∫—Ä–µ–º—É –æ–∑–Ω–∞–∫—É** —É —Ç–∞–±–ª–∏—á–Ω—ñ–π –º–æ–¥–µ–ª—ñ.\n",
    "* –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ **feature extraction** (–≤–∏—Ç—è–≥–Ω—É—Ç–∏ embeddings –∑ CNN —ñ –ø–µ—Ä–µ–¥–∞—Ç–∏ –≤ –∞–Ω—Å–∞–º–±–ª–µ–≤—É –º–æ–¥–µ–ª—å —Ä–∞–∑–æ–º —ñ–∑ —Ç–µ–∫—Å—Ç–æ–≤–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏).\n",
    "\n",
    "### 2. –¢–µ–∫—Å—Ç–∏\n",
    "\n",
    "* –ü–µ—Ä–µ–π—Ç–∏ –≤—ñ–¥ TF-IDF –¥–æ **BERT-–ø–æ–¥—ñ–±–Ω–∏—Ö –º–æ–¥–µ–ª–µ–π** –¥–ª—è —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è sentence embeddings.\n",
    "* –î–æ–¥–∞—Ç–∏ –ø—Ä–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫–∏: –¥–æ–≤–∂–∏–Ω–∞ –æ–ø–∏—Å—É, –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤, –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–∏–º–≤–æ–ª—ñ–≤.\n",
    "* –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ LightGBM / CatBoost –ø–æ–≤–µ—Ä—Ö TF-IDF + –º–µ—Ç–∞–¥–∞–Ω–∏—Ö.\n",
    "\n",
    "### 3. –ê–Ω—Å–∞–º–±–ª—å\n",
    "\n",
    "* –ó–∞–º—ñ—Å—Ç—å —Ñ—ñ–∫—Å–æ–≤–∞–Ω–∏—Ö –≤–∞–≥ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏:\n",
    "\n",
    "  * **–ú–µ—Ç–∞-–º–æ–¥–µ–ª—å (stacking):** –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è —á–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π –±—É—Å—Ç–∏–Ω–≥, —è–∫–∞ –≤—á–∏—Ç—å—Å—è –ø–æ—î–¥–Ω—É–≤–∞—Ç–∏ `proba_text` + `proba_img`.\n",
    "  * **–ê–¥–∞–ø—Ç–∏–≤–Ω—ñ –≤–∞–≥–∏:** –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ñ–æ—Ç–æ –∞–±–æ –≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ –º–æ–¥–µ–ª—ñ.\n",
    "* –ú–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –ø—Ä–æ—Å—Ç–∏–π ‚Äúconfidence gating‚Äù: —è–∫—â–æ —Ñ–æ—Ç–æ-–º–æ–¥–µ–ª—å –≤–ø–µ–≤–Ω–µ–Ω–∞ (>0.8), –¥–∞—Ç–∏ —ó–π –±—ñ–ª—å—à—É –≤–∞–≥—É.\n",
    "\n",
    "### 4. –Ü–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏\n",
    "\n",
    "* –í–µ—Å—Ç–∏ **—á—ñ—Ç–∫–∏–π —Ç—Ä–µ–∫—ñ–Ω–≥ –µ–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ñ–≤** (MLflow, Weights\\&Biases).\n",
    "* –ó—Ä–æ–±–∏—Ç–∏ **–≤–∞–ª—ñ–¥–∞—Ü—ñ—é –∑ —Ä–æ–∑–±–∏—Ç—Ç—è–º –ø–æ PetID** (—â–æ–± —Ñ–æ—Ç–æ –æ–¥–Ω—ñ—î—ó —Ç–≤–∞—Ä–∏–Ω–∏ –Ω–µ –ø–æ—Ç—Ä–∞–ø–ª—è–ª–∏ —ñ –≤ train, —ñ –≤ val).\n",
    "* –í–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ **stratified K-fold cross-validation** –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–∏—Ö –æ—Ü—ñ–Ω–æ–∫.\n",
    "\n",
    "## –í–∏—Å–Ω–æ–≤–∫–∏\n",
    "\n",
    "* –¢–µ–∫—Å—Ç–æ–≤–∞ –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–ª–∞ —Å–∏–ª—å–Ω–∏–π baseline.\n",
    "* –Ø–∫—ñ—Å—Ç—å –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–º—É —Ç–µ—Å—Ç—ñ –æ–±–º–µ–∂–µ–Ω–∞ —Å–ª–∞–±–∫–æ—é –æ–±—Ä–æ–±–∫–æ—é –∑–æ–±—Ä–∞–∂–µ–Ω—å.\n",
    "* –û—Å–Ω–æ–≤–Ω–∏–π –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è ‚Äî **–ø–æ–≥–ª–∏–±–ª–µ–Ω–∞ —Ä–æ–±–æ—Ç–∞ –∑ —Ñ–æ—Ç–æ**, –∞ —Ç–∞–∫–æ–∂ –ø–µ—Ä–µ—Ö—ñ–¥ –¥–æ –±—ñ–ª—å—à –ø–æ—Ç—É–∂–Ω–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –º–æ–¥–µ–ª–µ–π.\n",
    "* –£ –º–∞–π–±—É—Ç–Ω—ñ—Ö —ñ—Ç–µ—Ä–∞—Ü—ñ—è—Ö –≤–∞—Ä—Ç–æ —Ä–æ–∑–≥–ª—è–¥–∞—Ç–∏ –∞–Ω—Å–∞–º–±–ª—ñ, —è–∫—ñ –ø–æ—î–¥–Ω—É—é—Ç—å CNN-–µ–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω–∏–º–∏ —Ç–µ–∫—Å—Ç–æ–≤–∏–º–∏ –µ–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —ñ –¥–æ–¥–∞—Ç–∫–æ–≤–∏–º–∏ —Ç–∞–±–ª–∏—á–Ω–∏–º–∏ –æ–∑–Ω–∞–∫–∞–º–∏ (–∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ–æ—Ç–æ, –¥–æ–≤–∂–∏–Ω–∞ –æ–ø–∏—Å—É, –≤—ñ–∫/–ø–æ—Ä–æ–¥–∞).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd4be25",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
